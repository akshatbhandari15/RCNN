{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dirty-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proof-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['37261', '37913', '37914', '37915', '37916']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "computational-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newsgroups: comp.graphics\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
      "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
      "Subject: Looking for Brain in CAD\n",
      "Message-ID: <c285m+p@rpi.edu>\n",
      "Nntp-Posting-Host: nason110.its.rpi.edu\n",
      "Reply-To: mabusj@rpi.edu\n",
      "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
      "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
      "Lines: 7\n",
      "\n",
      "Jasen Mabus\n",
      "RPI student\n",
      "\n",
      "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
      "\n",
      "Thank you in advance,\n",
      "Jasen Mabus  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(data_dir / \"comp.graphics\" / \"38987\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "periodic-terrace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electoral-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "direct-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desirable-basement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "personalized-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 3457, 1682,   15,    2, 5776], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "friendly-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = r\"C:\\Users\\aksha\\Desktop\\TensorFlow\\glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mature-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "answering-services",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18019 words (1981 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ranking-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assured-princeton",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling1d_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-31065ced753d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[1;31m# >> model = tf.keras.Model(inputs, outputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 925\u001b[1;33m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0m\u001b[0;32m    926\u001b[0m                                                 input_list)\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1090\u001b[0m       \u001b[1;31m# TODO(reedwm): We should assert input compatibility after the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m       \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m       \u001b[0minput_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Use `self._name_scope()` to avoid auto-incrementing the name.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    174\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0m\u001b[0;32m    177\u001b[0m                          \u001b[0mlayer_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer max_pooling1d_8 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 128]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "x = layers.Activation('tanh')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "neural-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "becoming-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 11s 89ms/step - loss: 2.6637 - acc: 0.1337 - val_loss: 2.1157 - val_acc: 0.2806\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 11s 90ms/step - loss: 1.9728 - acc: 0.3208 - val_loss: 1.5200 - val_acc: 0.4969\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 1.5193 - acc: 0.4787 - val_loss: 1.4786 - val_acc: 0.4771\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 10s 81ms/step - loss: 1.2871 - acc: 0.5573 - val_loss: 1.1659 - val_acc: 0.5979\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 11s 84ms/step - loss: 1.1313 - acc: 0.6101 - val_loss: 1.0666 - val_acc: 0.6294\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 10s 83ms/step - loss: 1.0013 - acc: 0.6534 - val_loss: 1.0767 - val_acc: 0.6372\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.8909 - acc: 0.6900 - val_loss: 1.0886 - val_acc: 0.6357\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 11s 86ms/step - loss: 0.7824 - acc: 0.7265 - val_loss: 1.0321 - val_acc: 0.6687\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 12s 93ms/step - loss: 0.6936 - acc: 0.7537 - val_loss: 1.0252 - val_acc: 0.6667\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.6146 - acc: 0.7837 - val_loss: 0.9892 - val_acc: 0.6772\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 10s 84ms/step - loss: 0.5339 - acc: 0.8119 - val_loss: 1.1954 - val_acc: 0.6507\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.4666 - acc: 0.8363 - val_loss: 1.0750 - val_acc: 0.7007\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 12s 95ms/step - loss: 0.4077 - acc: 0.8568 - val_loss: 1.1590 - val_acc: 0.6839\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 12s 94ms/step - loss: 0.3516 - acc: 0.8792 - val_loss: 1.2836 - val_acc: 0.6894\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.3177 - acc: 0.8930 - val_loss: 1.1277 - val_acc: 0.7124\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 11s 88ms/step - loss: 0.2894 - acc: 0.9044 - val_loss: 1.2140 - val_acc: 0.6792\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 11s 85ms/step - loss: 0.2455 - acc: 0.9165 - val_loss: 1.1771 - val_acc: 0.7019\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 10s 80ms/step - loss: 0.2242 - acc: 0.9240 - val_loss: 1.2566 - val_acc: 0.7127\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 10s 82ms/step - loss: 0.2056 - acc: 0.9316 - val_loss: 1.3234 - val_acc: 0.7077\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 10s 79ms/step - loss: 0.1952 - acc: 0.9345 - val_loss: 1.3538 - val_acc: 0.7089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d2625df070>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-payday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

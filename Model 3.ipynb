{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stopped-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    \"news20.tar.gz\",\n",
    "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
    "    untar=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pacific-patent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of directories: 20\n",
      "Directory names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of files in comp.graphics: 1000\n",
      "Some example filenames: ['37261', '37913', '37914', '37915', '37916']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
    "dirnames = os.listdir(data_dir)\n",
    "print(\"Number of directories:\", len(dirnames))\n",
    "print(\"Directory names:\", dirnames)\n",
    "\n",
    "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
    "print(\"Number of files in comp.graphics:\", len(fnames))\n",
    "print(\"Some example filenames:\", fnames[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "likely-session",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing alt.atheism, 1000 files found\n",
      "Processing comp.graphics, 1000 files found\n",
      "Processing comp.os.ms-windows.misc, 1000 files found\n",
      "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
      "Processing comp.sys.mac.hardware, 1000 files found\n",
      "Processing comp.windows.x, 1000 files found\n",
      "Processing misc.forsale, 1000 files found\n",
      "Processing rec.autos, 1000 files found\n",
      "Processing rec.motorcycles, 1000 files found\n",
      "Processing rec.sport.baseball, 1000 files found\n",
      "Processing rec.sport.hockey, 1000 files found\n",
      "Processing sci.crypt, 1000 files found\n",
      "Processing sci.electronics, 1000 files found\n",
      "Processing sci.med, 1000 files found\n",
      "Processing sci.space, 1000 files found\n",
      "Processing soc.religion.christian, 997 files found\n",
      "Processing talk.politics.guns, 1000 files found\n",
      "Processing talk.politics.mideast, 1000 files found\n",
      "Processing talk.politics.misc, 1000 files found\n",
      "Processing talk.religion.misc, 1000 files found\n",
      "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Number of samples: 19997\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "labels = []\n",
    "class_names = []\n",
    "class_index = 0\n",
    "for dirname in sorted(os.listdir(data_dir)):\n",
    "    class_names.append(dirname)\n",
    "    dirpath = data_dir / dirname\n",
    "    fnames = os.listdir(dirpath)\n",
    "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
    "    for fname in fnames:\n",
    "        fpath = dirpath / fname\n",
    "        f = open(fpath, encoding=\"latin-1\")\n",
    "        content = f.read()\n",
    "        lines = content.split(\"\\n\")\n",
    "        lines = lines[10:]\n",
    "        content = \"\\n\".join(lines)\n",
    "        samples.append(content)\n",
    "        labels.append(class_index)\n",
    "    class_index += 1\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Number of samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "selected-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(samples)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "timely-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = tensorflow.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competitive-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generous-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = r\"C:\\Users\\aksha\\Desktop\\TensorFlow\\glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hawaiian-vancouver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "descending-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 18019 words (1981 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "palestinian-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "continued-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 128)         84480     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 2,103,772\n",
      "Trainable params: 103,572\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "x = layers.Activation('tanh')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "challenging-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intensive-entry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "125/125 [==============================] - 54s 429ms/step - loss: 2.3389 - acc: 0.2792 - val_loss: 1.6745 - val_acc: 0.4551\n",
      "Epoch 2/40\n",
      "125/125 [==============================] - 50s 399ms/step - loss: 1.4103 - acc: 0.5421 - val_loss: 1.2429 - val_acc: 0.5911\n",
      "Epoch 3/40\n",
      "125/125 [==============================] - 50s 404ms/step - loss: 1.1316 - acc: 0.6260 - val_loss: 1.0786 - val_acc: 0.6387\n",
      "Epoch 4/40\n",
      "125/125 [==============================] - 51s 408ms/step - loss: 0.9975 - acc: 0.6696 - val_loss: 1.0159 - val_acc: 0.6429\n",
      "Epoch 5/40\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.8970 - acc: 0.7048 - val_loss: 0.9398 - val_acc: 0.6804\n",
      "Epoch 6/40\n",
      "125/125 [==============================] - 51s 407ms/step - loss: 0.8192 - acc: 0.7281 - val_loss: 0.9173 - val_acc: 0.6917\n",
      "Epoch 7/40\n",
      "125/125 [==============================] - 54s 430ms/step - loss: 0.7564 - acc: 0.7477 - val_loss: 0.8805 - val_acc: 0.7069\n",
      "Epoch 8/40\n",
      "125/125 [==============================] - 52s 415ms/step - loss: 0.6955 - acc: 0.7697 - val_loss: 0.8411 - val_acc: 0.7209\n",
      "Epoch 9/40\n",
      "125/125 [==============================] - 52s 420ms/step - loss: 0.6383 - acc: 0.7879 - val_loss: 0.8232 - val_acc: 0.7217\n",
      "Epoch 10/40\n",
      "125/125 [==============================] - 51s 410ms/step - loss: 0.5938 - acc: 0.8060 - val_loss: 0.8242 - val_acc: 0.7284\n",
      "Epoch 11/40\n",
      "125/125 [==============================] - 52s 414ms/step - loss: 0.5558 - acc: 0.8168 - val_loss: 0.8062 - val_acc: 0.7387\n",
      "Epoch 12/40\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 0.5073 - acc: 0.8335 - val_loss: 0.7887 - val_acc: 0.7434\n",
      "Epoch 13/40\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 0.4741 - acc: 0.8449 - val_loss: 0.7899 - val_acc: 0.7457\n",
      "Epoch 14/40\n",
      "125/125 [==============================] - 53s 428ms/step - loss: 0.4333 - acc: 0.8596 - val_loss: 0.7916 - val_acc: 0.7444\n",
      "Epoch 15/40\n",
      "125/125 [==============================] - 52s 417ms/step - loss: 0.3990 - acc: 0.8716 - val_loss: 0.8089 - val_acc: 0.7434\n",
      "Epoch 16/40\n",
      "125/125 [==============================] - 53s 424ms/step - loss: 0.3661 - acc: 0.8816 - val_loss: 0.8293 - val_acc: 0.7357\n",
      "Epoch 17/40\n",
      "125/125 [==============================] - 52s 416ms/step - loss: 0.3419 - acc: 0.8909 - val_loss: 0.7884 - val_acc: 0.7557\n",
      "Epoch 18/40\n",
      "125/125 [==============================] - 52s 419ms/step - loss: 0.3102 - acc: 0.9020 - val_loss: 0.8061 - val_acc: 0.7524\n",
      "Epoch 19/40\n",
      "125/125 [==============================] - 55s 436ms/step - loss: 0.2850 - acc: 0.9090 - val_loss: 0.7964 - val_acc: 0.7522\n",
      "Epoch 20/40\n",
      "125/125 [==============================] - 56s 447ms/step - loss: 0.2559 - acc: 0.9221 - val_loss: 0.8461 - val_acc: 0.7464\n",
      "Epoch 21/40\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.2366 - acc: 0.9247 - val_loss: 0.8173 - val_acc: 0.7582\n",
      "Epoch 22/40\n",
      "125/125 [==============================] - 56s 445ms/step - loss: 0.2171 - acc: 0.9315 - val_loss: 0.8389 - val_acc: 0.7537\n",
      "Epoch 23/40\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.2006 - acc: 0.9372 - val_loss: 0.8686 - val_acc: 0.7459\n",
      "Epoch 24/40\n",
      "125/125 [==============================] - 53s 426ms/step - loss: 0.1774 - acc: 0.9440 - val_loss: 0.8840 - val_acc: 0.7489\n",
      "Epoch 25/40\n",
      "125/125 [==============================] - 55s 437ms/step - loss: 0.1706 - acc: 0.9456 - val_loss: 0.8977 - val_acc: 0.7452\n",
      "Epoch 26/40\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 0.1505 - acc: 0.9535 - val_loss: 0.9270 - val_acc: 0.7567\n",
      "Epoch 27/40\n",
      "125/125 [==============================] - 55s 440ms/step - loss: 0.1434 - acc: 0.9531 - val_loss: 0.9253 - val_acc: 0.7542\n",
      "Epoch 28/40\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.1409 - acc: 0.9539 - val_loss: 0.9276 - val_acc: 0.7567\n",
      "Epoch 29/40\n",
      "125/125 [==============================] - 54s 433ms/step - loss: 0.1257 - acc: 0.9594 - val_loss: 0.9837 - val_acc: 0.7489\n",
      "Epoch 30/40\n",
      "125/125 [==============================] - 56s 444ms/step - loss: 0.1227 - acc: 0.9592 - val_loss: 0.9834 - val_acc: 0.7427\n",
      "Epoch 31/40\n",
      "125/125 [==============================] - 56s 446ms/step - loss: 0.1300 - acc: 0.9564 - val_loss: 0.9856 - val_acc: 0.7464\n",
      "Epoch 32/40\n",
      "125/125 [==============================] - 55s 439ms/step - loss: 0.1156 - acc: 0.9611 - val_loss: 0.9963 - val_acc: 0.7497\n",
      "Epoch 33/40\n",
      "125/125 [==============================] - 56s 449ms/step - loss: 0.1165 - acc: 0.9602 - val_loss: 0.9822 - val_acc: 0.7544\n",
      "Epoch 34/40\n",
      "125/125 [==============================] - 56s 444ms/step - loss: 0.1016 - acc: 0.9646 - val_loss: 0.9949 - val_acc: 0.7559\n",
      "Epoch 35/40\n",
      "125/125 [==============================] - 54s 436ms/step - loss: 0.1011 - acc: 0.9654 - val_loss: 1.0272 - val_acc: 0.7542\n",
      "Epoch 36/40\n",
      "125/125 [==============================] - 55s 444ms/step - loss: 0.1044 - acc: 0.9643 - val_loss: 1.0364 - val_acc: 0.7549\n",
      "Epoch 37/40\n",
      "125/125 [==============================] - 55s 442ms/step - loss: 0.0978 - acc: 0.9641 - val_loss: 1.0450 - val_acc: 0.7527\n",
      "Epoch 38/40\n",
      "125/125 [==============================] - 54s 435ms/step - loss: 0.0928 - acc: 0.9672 - val_loss: 1.0556 - val_acc: 0.7549\n",
      "Epoch 39/40\n",
      "125/125 [==============================] - 53s 427ms/step - loss: 0.0985 - acc: 0.9656 - val_loss: 1.0701 - val_acc: 0.7454\n",
      "Epoch 40/40\n",
      "125/125 [==============================] - 55s 438ms/step - loss: 0.0890 - acc: 0.9667 - val_loss: 1.0776 - val_acc: 0.7544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2048ecd1d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=40, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "imposed-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model 3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-grounds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
